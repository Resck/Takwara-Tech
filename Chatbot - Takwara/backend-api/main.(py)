# -*- coding: utf-8 -*-
# VERSÃO FINAL: Inclui uma função de limpeza na saída para remover
# de forma definitiva qualquer vazamento de fontes na resposta final.

import os
import functions_framework
from flask import Flask, request, jsonify, make_response
from langchain_community.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import traceback

app = Flask(__name__)

# --- Variáveis Globais (Componentes da IA) ---
qa_chain = None
vector_store = None
llm = None

# --- Constantes ---
PERSIST_DIRECTORY = './chroma_db' 

# --- Template de Prompt (O "Cérebro" do Assistente) ---
prompt_template = """
You are a helpful and cordial virtual assistant Takwara, an expert in sustainable soluction for use bamboo and socio-environmental responsibility, against the global climate crise.
Your mission is to answer questions based ONLY on the information provided in the following context documents about the "Tecnologia Takwara" project.
Analyze the user's question to determine its language. YOU MUST RESPOND IN THE SAME LANGUAGE AS THE QUESTION.
If the user's question has spelling or grammatical errors, interpret it to the best of your ability and answer the most likely intended question.
If the answer to a user's question is not found within the provided context documents, politely state this limitation in the user's language. DO NOT invent answers.
Your responses must be formatted using **Markdown** to improve readability and understanding.
When referencing the project's origin or creator, mention Fabio "Takwara" Resck as the idealizer, acknowledging the collective "Nós" as explained in the text.
Maintain a positive, cordial, encouraging, and informative tone consistent with the document's spirit.
IMPORTANT: You must never, under any circumstances, cite or mention file names, paths, or sources in your response. Your answer should be a direct, helpful text.

Context (Documents):
{context}

Question:
{question}

Helpful and cordial answer in the user's language (without citing sources):
"""
PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)

# --- NOVA FUNÇÃO DE LIMPEZA ---
# Esta função recebe o texto gerado pela IA e remove as linhas indesejadas.
def clean_response(text: str) -> str:
    """
    Limpa a resposta final da IA, removendo linhas de fontes e outras
    informações indesejadas que possam ter vazado.
    """
    # Lista de prefixos de linhas a serem removidas
    unwanted_prefixes = [
        "Fonte(s):",
        "(Fontes:",
        "Fontes:",
        "Lembre-se que este material"
    ]
    
    # Divide a resposta em linhas
    lines = text.splitlines()
    
    # Cria uma nova lista contendo apenas as linhas que NÃO começam com os prefixos indesejados
    cleaned_lines = [line for line in lines if not any(line.strip().startswith(prefix) for prefix in unwanted_prefixes)]
    
    # Junta as linhas limpas de volta em um único texto
    return "\n".join(cleaned_lines)

# --- Função de Carregamento ---
def load_components():
    """
    Carrega e inicializa todos os componentes de IA.
    """
    print("--- EXECUTANDO VERSÃO FINAL (com limpeza de resposta) ---") 
    global qa_chain, vector_store, llm
    try:
        if not os.getenv("GOOGLE_API_KEY"):
             print("ERRO CRÍTICO: GOOGLE_API_KEY não encontrada.")
             return
        print("A carregar componentes de IA...")
        llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro-latest", temperature=0.3)
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        if not os.path.exists(PERSIST_DIRECTORY):
            print(f"ERRO CRÍTICO: Diretório ChromaDB não encontrado em '{PERSIST_DIRECTORY}'.")
            return
        vector_store = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings)
        retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 8})
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm, chain_type="stuff", retriever=retriever,
            chain_type_kwargs={"prompt": PROMPT}, return_source_documents=False
        )
        print(">>> Componentes de IA carregados com sucesso! <<<")
    except Exception as e:
        print(f"ERRO CRÍTICO GERAL DURANTE O CARREGAMENTO: {e}")
        traceback.print_exc()

# --- Função Principal da API ---
@functions_framework.http
def chatbot_api(request):
    """
    Manipulador de requisições HTTP.
    """
    headers = {'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Methods': 'POST, GET, OPTIONS', 'Access-Control-Allow-Headers': 'Content-Type'}
    if request.method == 'OPTIONS':
        return ('', 204, headers)

    if qa_chain is None:
        load_components()
        if qa_chain is None:
             return (jsonify({"error": "Falha crítica ao carregar o sistema de IA."}), 500, headers)

    if request.method == 'POST':
        try:
            request_json = request.get_json(silent=True)
            if not request_json or 'query' not in request_json:
                return (jsonify({"error": "Pedido inválido."}), 400, headers)
            query = request_json['query']
            result = qa_chain.invoke({"query": query})
            answer = result.get('result', 'Não foi possível gerar uma resposta.')
            
            # --- APLICA A LIMPEZA ANTES DE ENVIAR A RESPOSTA ---
            cleaned_answer = clean_response(answer)
            
            return (jsonify({"answer": cleaned_answer}), 200, headers)
        except Exception as e:
            print(f"ERRO INESPERADO: {e}")
            traceback.print_exc()
            return (jsonify({"error": "Ocorreu um erro interno."}), 500, headers)
    else:
        return (jsonify({"error": f"Método '{request.method}' não permitido."}), 405, headers)