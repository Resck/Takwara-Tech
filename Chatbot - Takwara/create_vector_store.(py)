# -*- coding: utf-8 -*-
"""
VERSÃO 4.3 (FINAL): Aumenta o tamanho do chunk e remove a chave 'source' de TODOS
os metadados como passo final para garantir que não haja vazamento de fontes.
"""
import os
import shutil
import io
import tempfile
from dotenv import load_dotenv
from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from google.cloud import storage
from urllib.parse import quote
from pathlib import Path

# --- Configurações ---
load_dotenv()

GCS_BUCKET_NAME = os.getenv("GCS_BUCKET_NAME", "takwara-bank")
DOCS_ROOT_FOLDER = './docs'
PERSIST_DIRECTORY = './backend-api/chroma_db' 
BASE_URL_SITE = "https://resck.github.io/Takwara-Tech/"
GENERIC_GCS_URL = f"https://storage.googleapis.com/{GCS_BUCKET_NAME}/"

MARKDOWN_HEADERS_TO_SPLIT_ON = [("#", "Header 1"), ("##", "Header 2"), ("###", "Header 3")]

# --- MUDANÇA PRINCIPAL 1: AUMENTO DO CHUNK ---
# Aumenta o tamanho dos pedaços para capturar mais contexto em cada um.
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200

# --- Funções Auxiliares (sem alterações) ---
def get_url_from_file_path(file_path):
    try:
        p = Path(file_path)
        relative_p = p.relative_to(DOCS_ROOT_FOLDER)
        path_without_ext = relative_p.with_suffix('')
        url_parts = [quote(part) for part in path_without_ext.parts]
        url_path = "/".join(url_parts)
        final_url = f"{BASE_URL_SITE}{url_path}/"
        return final_url
    except Exception as e:
        print(f"Aviso: Erro ao gerar URL para '{file_path}': {e}. Usando URL base.")
        return BASE_URL_SITE

def load_docs_from_gcs(bucket_name):
    print(f"\n--- A carregar documentos PDF do bucket '{bucket_name}' no GCS ---")
    try:
        storage_client = storage.Client()
        bucket = storage_client.get_bucket(bucket_name)
        blobs = list(bucket.list_blobs())
        pdf_docs = []
        pdf_count = len([b for b in blobs if b.name.lower().endswith('.pdf')])
        if pdf_count == 0:
            print("Aviso: Nenhum arquivo PDF encontrado no bucket.")
            return []
            
        for blob in blobs:
            if blob.name.lower().endswith('.pdf'):
                print(f"  -> A processar: {blob.name}")
                temp_file_path = None
                try:
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
                        blob.download_to_filename(temp_file.name)
                        temp_file_path = temp_file.name
                    documents = PyPDFLoader(file_path=temp_file_path).load()
                    for doc in documents:
                        doc.metadata['source'] = os.path.basename(blob.name)
                    pdf_docs.extend(documents)
                finally:
                    if temp_file_path: os.remove(temp_file_path)
        print(f"Sucesso! Carregados {len(pdf_docs)} páginas de {pdf_count} PDFs.")
        return pdf_docs
    except Exception as e:
        print(f"ERRO CRÍTICO ao acessar o bucket '{bucket_name}': {e}")
        return []

# --- Função Principal ---
def build_and_save_vector_store():
    print("--- INICIANDO CRIAÇÃO DA BASE DE VETORES (VERSÃO 4.3) ---")

    # ETAPA 1: CARREGAR DOCUMENTOS
    print("\n[ETAPA 1/5] Carregando documentos de todas as fontes...")
    loader_md = DirectoryLoader(DOCS_ROOT_FOLDER, glob="**/*.md", loader_cls=TextLoader, recursive=True, show_progress=True)
    markdown_docs = loader_md.load()
    pdf_docs = load_docs_from_gcs(GCS_BUCKET_NAME)
    all_docs = markdown_docs + pdf_docs
    print(f"Total de documentos carregados: {len(all_docs)}")

    # ETAPA 2: ADICIONAR METADADOS DE URL
    print("\n[ETAPA 2/5] Adicionando metadados de URL...")
    for doc in all_docs:
        source_path = doc.metadata.get('source', '')
        if source_path.lower().endswith('.md'):
            doc.metadata['url'] = get_url_from_file_path(source_path)
        else: 
            doc.metadata['url'] = GENERIC_GCS_URL
    print("Metadados de URL adicionados.")

    # ETAPA 3: DIVIDIR DOCUMENTOS EM PEDAÇOS (CHUNKS)
    print("\n[ETAPA 3/5] Dividindo documentos em pedaços (chunks)...")
    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=MARKDOWN_HEADERS_TO_SPLIT_ON)
    recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    chunks = []
    for doc in all_docs:
        source_is_md = doc.metadata.get('source', '').lower().endswith('.md')
        
        # A lógica de propagar os metadados para os chunks permanece a mesma.
        # Agora, os chunks dos arquivos .md não terão a chave 'source'.
        if source_is_md:
            doc_chunks = markdown_splitter.split_text(doc.page_content)
            for chunk in doc_chunks:
                chunk.metadata = doc.metadata.copy()
            chunks.extend(doc_chunks)
        else: # Documentos PDF
            pdf_chunks = recursive_splitter.split_documents([doc])
            chunks.extend(pdf_chunks)
    print(f"Divisão concluída. Total de chunks gerados: {len(chunks)}")

    # --- MUDANÇA PRINCIPAL 2: REMOÇÃO FINAL DAS FONTES ---
    # ETAPA 4: LIMPEZA FINAL DE METADADOS
    print("\n[ETAPA 4/5] Limpando metadados 'source' de todos os chunks...")
    for chunk in chunks:
        if 'source' in chunk.metadata:
            del chunk.metadata['source']
    print("Limpeza final de metadados concluída.")

    # ETAPA 5: GERAR EMBEDDINGS E SALVAR NO CHROMADB
    print(f"\n[ETAPA 5/5] Gerando embeddings e salvando no ChromaDB...")
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        if os.path.exists(PERSIST_DIRECTORY):
            print(f"Removendo base de dados antiga em '{PERSIST_DIRECTORY}'...")
            shutil.rmtree(PERSIST_DIRECTORY)
        
        Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=PERSIST_DIRECTORY)
        print(f"\n--- SUCESSO! Nova base de vetores criada em '{PERSIST_DIRECTORY}'! ---")
    except Exception as e:
        print(f"\nERRO FATAL na etapa final: {e}")
        exit(1)

if __name__ == "__main__":
    build_and_save_vector_store()